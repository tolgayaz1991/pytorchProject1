{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb7fc90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbece2f",
   "metadata": {},
   "source": [
    "# 0. Deep Learning with PyTorch 1 (Project 1, Cancer Diagnosis)\n",
    "\n",
    "# Goal: \n",
    "\n",
    "## Cancer diagnosis based on the input feature values (usage of logistic reg).\n",
    "\n",
    "# _00. Import Main Libraries_\n",
    "\n",
    "# _01. Load the Data and Do the Preprocess Step_\n",
    "\n",
    "# _02. Modelling_\n",
    "\n",
    "# _03. Loss Function_\n",
    "\n",
    "# _04. Optimizer_\n",
    "\n",
    "# _05. Training Loop_\n",
    "\n",
    "# _06. Success Evaluation (Accuracy)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef538b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5c1c6",
   "metadata": {},
   "source": [
    "# _00. Import Main Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df503a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d820d99",
   "metadata": {},
   "source": [
    "# _01. Load the Data and Do the Preprocessing Step_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7767de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "bc = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752307ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 133.5 KB\n"
     ]
    }
   ],
   "source": [
    "# explore the dataset as DataFrame\n",
    "\n",
    "bcDf = pd.DataFrame(bc.data, columns = bc.feature_names)\n",
    "\n",
    "bcDf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c70b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and target specification\n",
    "\n",
    "X, y = bc.data, bc.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba04305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "# Sample (Observation) and Feature Numbers\n",
    "\n",
    "nSamples, nFeatures = X.shape\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83dc23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e870b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "X_train1 = scaler1.fit_transform(X_train)\n",
    "\n",
    "X_test1 = scaler1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3995fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the numpy data into tensor\n",
    "\n",
    "X_train1_tso = th.from_numpy(X_train1.astype(np.float32))\n",
    "\n",
    "X_test1_tso = th.from_numpy(X_test1.astype(np.float32))\n",
    "\n",
    "y_train_tso = th.from_numpy(y_train.astype(np.float32))\n",
    "\n",
    "y_test_tso = th.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07a88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tso = y_train_tso.view(-1,1)\n",
    "\n",
    "y_test_tso = y_test_tso.view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1600bb",
   "metadata": {},
   "source": [
    "# _02. Modelling (Logistic Reg)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00bd2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the logistic reg class\n",
    "\n",
    "class LogisticReg(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticReg, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward (self, X):\n",
    "        y_predicted = th.sigmoid(self.linear(X))\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e31a4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticReg(input_dim=nFeatures, output_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982acc14",
   "metadata": {},
   "source": [
    "# _03. Loss Function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae6011fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fLoss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7217170",
   "metadata": {},
   "source": [
    "# _04. Optimizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb82c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33592d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent for optimization\n",
    "\n",
    "optimizer = th.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080fa2c",
   "metadata": {},
   "source": [
    "# _05. Training Loop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "000baf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1091b53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss = 0.33187\n",
      "epoch: 10, loss = 0.32642\n",
      "epoch: 15, loss = 0.32124\n",
      "epoch: 20, loss = 0.31629\n",
      "epoch: 25, loss = 0.31157\n",
      "epoch: 30, loss = 0.30705\n",
      "epoch: 35, loss = 0.30273\n",
      "epoch: 40, loss = 0.29858\n",
      "epoch: 45, loss = 0.29461\n",
      "epoch: 50, loss = 0.29078\n",
      "epoch: 55, loss = 0.28711\n",
      "epoch: 60, loss = 0.28357\n",
      "epoch: 65, loss = 0.28016\n",
      "epoch: 70, loss = 0.27688\n",
      "epoch: 75, loss = 0.27371\n",
      "epoch: 80, loss = 0.27064\n",
      "epoch: 85, loss = 0.26768\n",
      "epoch: 90, loss = 0.26482\n",
      "epoch: 95, loss = 0.26205\n",
      "epoch: 100, loss = 0.25936\n",
      "epoch: 105, loss = 0.25676\n",
      "epoch: 110, loss = 0.25423\n",
      "epoch: 115, loss = 0.25178\n",
      "epoch: 120, loss = 0.24941\n",
      "epoch: 125, loss = 0.24710\n",
      "epoch: 130, loss = 0.24485\n",
      "epoch: 135, loss = 0.24267\n",
      "epoch: 140, loss = 0.24054\n",
      "epoch: 145, loss = 0.23847\n",
      "epoch: 150, loss = 0.23646\n",
      "epoch: 155, loss = 0.23450\n",
      "epoch: 160, loss = 0.23259\n",
      "epoch: 165, loss = 0.23072\n",
      "epoch: 170, loss = 0.22890\n",
      "epoch: 175, loss = 0.22713\n",
      "epoch: 180, loss = 0.22540\n",
      "epoch: 185, loss = 0.22371\n",
      "epoch: 190, loss = 0.22205\n",
      "epoch: 195, loss = 0.22044\n",
      "epoch: 200, loss = 0.21886\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nEpochs):\n",
    "    \n",
    "    # forward pass\n",
    "    yPredicted = model(X_train1_tso)\n",
    "    \n",
    "    # loss\n",
    "    loss = fLoss(yPredicted, y_train_tso)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # print some results to see the change\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"epoch: {epoch + 1}, loss = {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6321b2",
   "metadata": {},
   "source": [
    "# _06. Success Evaluation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccc310d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.92105\n"
     ]
    }
   ],
   "source": [
    "# without autograd\n",
    "\n",
    "with th.no_grad():\n",
    "    yPredicted = model(X_test1_tso)\n",
    "    yPredictedClasses = yPredicted.round()\n",
    "    accEval = yPredictedClasses.eq(y_test_tso).sum() / y_test_tso.shape[0]\n",
    "    print(f\"accuracy = {accEval:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729bf901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
